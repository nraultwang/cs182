{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLUs and Neural Network Intuition with different Optimizers\n",
    "\n",
    "**If you are running on a local anaconda install, you will need to install pytorch with the command**\n",
    "```sh\n",
    "conda install pytorch -c pytorch\n",
    "```\n",
    "\n",
    "<!-- #TODO(krishna) : add wandb integration -->\n",
    "<!-- You should immediately run all the cells up through 'Train all layers' since training the networks takes a long time. While you wait you can return to the theory portion of this discussion and work on the backpropagation problem. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title  Mount your Google Drive and set up mount symlink\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      5\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m DRIVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive/My\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m Drive/cs182fa25_public\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# !git clone https://github.com/Berkeley-CS182/cs182fa25_public.git\n",
    "\n",
    "# os.chdir('cs182fa25_public/dis02/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from ipywidgets import fixed, interactive, widgets \n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training and Test Data\n",
    "\n",
    "We are using piecewise linear function. Our training data has added noise $y = f(x) + \\epsilon,\\, \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The test data is noise free.\n",
    "\n",
    "_After you have gone through the discussion, you may wish to adjust the number of training samples and noise variance to see how gradient descent behaves under the new conditions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_type = 'piecewise_linear'\n",
    "\n",
    "def f_true(X, f_type):\n",
    "    if f_type == 'sin(20x)':\n",
    "        return np.sin(20 * X[:,0])\n",
    "    else:\n",
    "        TenX = 10 * X[:,0]\n",
    "        _ = 12345\n",
    "        return (TenX - np.floor(TenX)) * np.sin(_ * np.ceil(TenX)) - (TenX - np.ceil(TenX)) * np.sin(_ * np.floor(TenX)) \n",
    "    \n",
    "n_features = 1\n",
    "n_samples = 200\n",
    "sigma = 0.01\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Generate train data\n",
    "X = np.sort(rng.rand(n_samples, n_features), axis=0)\n",
    "y = f_true(X, f_type) + rng.randn(n_samples) * sigma\n",
    "\n",
    "# Generate NOISELESS test data\n",
    "X_test = np.concatenate([X.copy(), np.expand_dims(np.linspace(0., 1., 1000), axis=1)])\n",
    "X_test = np.sort(X_test, axis=0)\n",
    "y_test = f_true(X_test, f_type)\n",
    "\n",
    "# Save checkpoint files\n",
    "DIR_SGD = os.path.join(os.getcwd(), 'ckpts/sgd')\n",
    "DIR_SGDM = os.path.join(os.getcwd(), 'ckpts/sgd_momentum')\n",
    "DIR_ADAM = os.path.join(os.getcwd(), 'ckpts/adam')\n",
    "os.makedirs(DIR_SGD, exist_ok=True)\n",
    "os.makedirs(DIR_SGDM, exist_ok=True)\n",
    "os.makedirs(DIR_ADAM, exist_ok=True)\n",
    "\n",
    "def get_ckpt_dir(optim: str):\n",
    "    if optim == 'sgd':\n",
    "        return DIR_SGD\n",
    "    elif optim == 'sgd_momentum':\n",
    "        return DIR_SGDM\n",
    "    elif optim == 'adam':\n",
    "        return DIR_ADAM\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Networks\n",
    "\n",
    "We will learn the piecewise linear target function using a simple 1-hidden layer neural network with ReLU non-linearity, defined by\n",
    "$$ \\hat{y} = \\mathbf{W}^{(2)} \\Phi \\left( \\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)} \\right) + \\mathbf{b}^{(2)} $$\n",
    "where $\\Phi(x) = ReLU(x)$ and superscripts refer to indices, not the power operator.\n",
    "\n",
    "We will also create two SGD optimizers to allow us to choose whether to train all parameters or only the linear output layer's parameters. Note that we use separate learning rates for the two version of training. There is too much variance in the gradients when training all layers to use a large learning rate, so we have to decrease it.\n",
    "\n",
    "We will modify the default initialization of the biases so that the ReLU elbows are all inside the region we are interested in.\n",
    "\n",
    "We create several versions of this network with varying widths to explore how hidden layer width impacts learning performance.\n",
    "\n",
    "_Once you have gone through the discussion once you may wish to train networks with even larger widths to see how they behave under the three different training paradigms in this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m\n\u001b[1;32m     51\u001b[0m         nets_by_size[seed][width][optim] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m'\u001b[39m: network,\n\u001b[1;32m     53\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt_all\u001b[39m\u001b[38;5;124m'\u001b[39m: opt_all,\n\u001b[1;32m     54\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m'\u001b[39m: optim,\n\u001b[1;32m     55\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: seed\n\u001b[1;32m     56\u001b[0m         }\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m nn_seeds:\n\u001b[0;32m---> 59\u001b[0m   \u001b[43msetup_networks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_widths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36msetup_networks\u001b[0;34m(widths, optimizer, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Adjust initial biases so elbows are in [0,1]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m elbows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(width)) \n\u001b[0;32m---> 39\u001b[0m new_biases \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43melbows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m weights_all[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m to_torch(new_biases)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Create SGD optimizers for outputs alone and for all weights\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# lr_out = 0.2\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/180/lib/python3.10/site-packages/numpy/core/_internal.py:852\u001b[0m, in \u001b[0;36marray_ufunc_errmsg_formatter\u001b[0;34m(dummy, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_ufunc_errmsg_formatter\u001b[39m(dummy, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Format the error message for when __array_ufunc__ gives up. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     args_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m inputs] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    853\u001b[0m                             [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, v)\n\u001b[1;32m    854\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    855\u001b[0m     args \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m, ())\n\u001b[1;32m    856\u001b[0m     types_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[0;32m~/anaconda3/envs/180/lib/python3.10/site-packages/numpy/core/_internal.py:852\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_ufunc_errmsg_formatter\u001b[39m(dummy, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Format the error message for when __array_ufunc__ gives up. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     args_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{!r}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m inputs] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    853\u001b[0m                             [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, v)\n\u001b[1;32m    854\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    855\u001b[0m     args \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m, ())\n\u001b[1;32m    856\u001b[0m     types_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[0;32m~/anaconda3/envs/180/lib/python3.10/site-packages/numpy/_core/arrayprint.py:1578\u001b[0m, in \u001b[0;36m_array_repr_implementation\u001b[0;34m(arr, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1576\u001b[0m     class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1578\u001b[0m skipdtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtype_is_implied\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m arr\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1580\u001b[0m prefix \u001b[38;5;241m=\u001b[39m class_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1581\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skipdtype \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/180/lib/python3.10/site-packages/numpy/_core/arrayprint.py:1514\u001b[0m, in \u001b[0;36mdtype_is_implied\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype_is_implied\u001b[39m(dtype):\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;124;03m    Determine if the given dtype is implied by the representation\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;124;03m    of its values.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m    array([1, 2, 3], dtype=int8)\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1514\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m format_options\u001b[38;5;241m.\u001b[39mget()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegacy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m113\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool:\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Don't rerun this cell after training or you will lose all your work\n",
    "nets_by_size = {}\n",
    "nn_widths = [10, 20, 40]\n",
    "nn_optimizer = ['sgd', \"sgd_momentum\", 'adam']\n",
    "nn_seeds = [\n",
    "  442, 370, 378, 892, 836, 209, 327, 316, 216, 308,\n",
    "  748, 934, 558, 546, 266, 808, 884, 818, 277, 979, \n",
    "  766, 274, 479, 325, 431, 971, 689, 871, 272, 704\n",
    "]\n",
    "\n",
    "def setup_networks(widths, optimizer, seed):\n",
    "  \n",
    "  torch.manual_seed(seed)\n",
    "  nets_by_size[seed] = dict()\n",
    "\n",
    "  for width in widths:\n",
    "\n",
    "      nets_by_size[seed][width] = dict()\n",
    "      # Define a 1-hidden layer ReLU nonlinearity network. \n",
    "      # Initialize outside the optimizer loop to keep weight init the same.\n",
    "      net = nn.Sequential(\n",
    "        nn.Linear(1, width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(width, 1)\n",
    "      )\n",
    "\n",
    "      for optim in optimizer:\n",
    "        \n",
    "        # Clone the network\n",
    "        network = copy.deepcopy(net)\n",
    "\n",
    "        # Get trainable parameters\n",
    "        weights_all = list(network.parameters())\n",
    "        \n",
    "        # Get the output weights alone\n",
    "        weights_out = weights_all[2:]\n",
    "        # Adjust initial biases so elbows are in [0,1]\n",
    "        elbows = np.sort(np.random.rand(width)) \n",
    "        new_biases = -elbows * to_numpy(weights_all[0].cpu()).ravel()\n",
    "        weights_all[1].data = to_torch(new_biases)\n",
    "        # Create SGD optimizers for outputs alone and for all weights\n",
    "        # lr_out = 0.2\n",
    "        lr_all = 0.02\n",
    "        if optim == 'sgd':\n",
    "          opt_all = torch.optim.SGD(params=weights_all, lr=lr_all)\n",
    "        elif optim == 'sgd_momentum':\n",
    "          opt_all = torch.optim.SGD(params=weights_all, lr=lr_all, momentum=0.9)\n",
    "        elif optim == 'adam':\n",
    "          opt_all = torch.optim.Adam(params=weights_all, lr=lr_all )\n",
    "        # opt_out = torch.optim.SGD(params=weights_out, lr=lr_out)\n",
    "        nets_by_size[seed][width][optim] = {\n",
    "          'net': network,\n",
    "          'opt_all': opt_all,\n",
    "          'optim': optim,\n",
    "          'seed': seed\n",
    "        }\n",
    "\n",
    "for s in nn_seeds:\n",
    "  setup_networks(nn_widths, nn_optimizer, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all the networks now - this will take a while!\n",
    "\n",
    "Uncomment the code cell below to train the models from scratch. \n",
    "\n",
    "You can expect training to take between 5 and 10 minutes depending on whether you run locally or on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps = 30000\n",
    "# save_every = 3000 #1000\n",
    "# t0 = time.time()\n",
    "\n",
    "# def train_all_seeds(widths, optims, seeds):\n",
    "  \n",
    "#     for w in widths:\n",
    "#       for i, optim in enumerate(optims):\n",
    "\n",
    "#         print(\"-\"*40)\n",
    "#         print(\"Width\", w, \"Optimizer\", optim)\n",
    "#         list_of_history = []\n",
    "\n",
    "#         print(f\"training with {len(seeds)} seeds...\")\n",
    "#         for seed in tqdm(seeds):\n",
    "#           net = nets_by_size[seed][w][optim]['net']\n",
    "#           opt_all = nets_by_size[seed][w][optim]['opt_all']\n",
    "\n",
    "#           save_dir = f'{get_ckpt_dir(optim)}/width{w}/seed{seed}/'\n",
    "#           os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#           history_all = train_network(X, y, X_test, y_test, \n",
    "#                                   net, optim=opt_all, \n",
    "#                                   n_steps=n_steps, save_every=save_every, \n",
    "#                                   verbose=False, optimizer=optim, seed=seed,\n",
    "#                                   ckpt_dir=save_dir)\n",
    "#           nets_by_size[seed][w][optim]['hist_all'] = history_all\n",
    "#           list_of_history.append(history_all)  \n",
    "      \n",
    "# train_all_seeds(widths=nn_widths, optims=nn_optimizer, seeds=nn_seeds)\n",
    "  \n",
    "# t1 = time.time()\n",
    "# print(\"-\"*40)\n",
    "# print(\"Trained all layers in %.1f minutes\" % ((t1 - t0) / 60))\n",
    "\n",
    "# # Compile the tensors into a dictionary\n",
    "# data_dict = {\n",
    "#     'X': X,\n",
    "#     'y': y,\n",
    "#     'X_test': X_test,\n",
    "#     'y_test': y_test\n",
    "# }\n",
    "\n",
    "# torch.save(data_dict, \"ckpts/tensors.pth\")\n",
    "# torch.save(nets_by_size, \"ckpts/nets_by_size.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below loads pretrained checkpoints for the models trained with SGD, SGD with momentum, and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data_dict = torch.load(\"ckpts/tensors.pth\", weights_only=False)\n",
    "X = loaded_data_dict['X']\n",
    "y = loaded_data_dict['y']\n",
    "X_test = loaded_data_dict['X_test']\n",
    "y_test = loaded_data_dict['y_test']\n",
    "del loaded_data_dict\n",
    "\n",
    "nets_by_size = torch.load(\"ckpts/nets_by_size.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for w in nn_widths:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for i, optim in enumerate(nn_optimizer):\n",
    "        \n",
    "        # uncomment below if you ran the previous block to train all models\n",
    "        # list_of_history = [nets_by_size[s][w][optim]['hist_all'] for s in nn_seeds]\n",
    "        \n",
    "        # uncomment below if you want to use pretrained weights\n",
    "        c = get_ckpt_dir(optim)\n",
    "        list_of_history = [\n",
    "            torch.load(\n",
    "                os.path.join(f'{get_ckpt_dir(optim)}/width{w}/seed{s}', 'ckpt_and_history.pt')\n",
    "            ) for s in nn_seeds\n",
    "        ]\n",
    "        plot_with_error_bar(list_of_history, optim=optim, plot_train=True, idx=i, ax=ax)\n",
    "    ax.set_title(f\"width {w}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for w in nn_widths:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for i, optim in enumerate(nn_optimizer):\n",
    "\n",
    "        # uncomment below if you ran the previous block to train all models\n",
    "        # list_of_history = [nets_by_size[s][w][optim]['hist_all'] for s in nn_seeds]\n",
    "        \n",
    "        # uncomment below if you want to use pretrained weights\n",
    "        c = get_ckpt_dir(optim)\n",
    "        list_of_history = [\n",
    "            torch.load(\n",
    "                os.path.join(f'{get_ckpt_dir(optim)}/width{w}/seed{s}', 'ckpt_and_history.pt')\n",
    "            ) for s in nn_seeds\n",
    "        ]\n",
    "        plot_with_error_bar(list_of_history, optim=optim, plot_test=True, idx=i, ax=ax)  \n",
    "    ax.set_title(f\"width {w}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ReLU elbow positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE = nn_seeds[0]\n",
    "for w in nn_widths:\n",
    "    for optim in nn_optimizer:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        net = nets_by_size[SAMPLE][w][optim]['net']\n",
    "        plot_update(X, y, X_test, y_test, net, optim=optim, ax=ax)\n",
    "        ax.set_title(f\"width {w}, {optim}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Questions\n",
    "\n",
    "Questions to consider while exploring the training process for all layer weights with different optimizers:\n",
    "- **How does the hidden layer width and different optimizers impact the learned function and test error?**\n",
    "\n",
    "- **What happens to the elbow locations using different optimizers during training?**\n",
    "- **Check out `helper.py` file. Are the circle dots on the test loss graphs above mean or median? Why not plot the other one?**\n",
    "\n",
    "-  **How are error bars for the test losses computed? Are the upper and lower marks maximum/minimum, standard deviation or something else? Does it make sense to plot standard deviation here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a single figure with a 3x3 grid of subplots\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axs array for easy indexing\n",
    "axs = axs.ravel()\n",
    "\n",
    "# Iterate through the widths and optimizers and plot in subplots\n",
    "for i, w in enumerate(nn_widths):\n",
    "    for j, optim in enumerate(nn_optimizer):\n",
    "        ax = axs[i * 3 + j]\n",
    "        net = nets_by_size[SAMPLE][w][optim]['net']\n",
    "        plot_update(X, y, X_test, y_test, net, optim=optim, ax=ax)\n",
    "        ax.set_title(f\"width {w}, {optim}\")\n",
    "\n",
    "# Adjust subplot layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
