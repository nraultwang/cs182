{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "7t6tuD6vGUsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Intro\n",
        "\n",
        "In this problem, we will implement key components of the Muon optimizer in PyTorch. Then,  we will compare Muon against SGD and AdamW empirically to demonstrate the benefits of Muon.\n",
        "\n",
        "We will train a sample CNN architecture on CIFAR10 image data (initialized below)."
      ],
      "metadata": {
        "id": "ssD7ZMwsRHTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample CNN for CIFAR-10\n",
        "class CIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Dataset & loader with augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Increase batch size for more realistic training\n",
        "batch_size = 128\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_ds = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "fp4WlumrKZMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Implementing Newton-Schulz\n",
        "\n",
        "The core of Muon is using matrix orthogonalization on a typical gradient update matrix. After orthogonalization:\n",
        "* Singular values become more uniform\n",
        "* Updates act across all directions in parameter space\n",
        "* The neural network can utilize its full parameter capacity (because parameters are all receiving nontrivial gradients)\n",
        "\n",
        "In Muon, matrix orthogonalization is done using Newton-Schulz. Newton-Schulz relies on iteration using an odd matrix polynomial. In this problem, we consider a very simple cubic polynomial, but many odd polynomials can be used (state-of-the-art implementations use specific, tuned quintic polynomial).\n",
        "\n",
        "**Complete the following code** to implement Newton-Schulz, using the matrix polynomial in the comments.\n"
      ],
      "metadata": {
        "id": "TjpGbmJRTPaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_schulz_orthogonalize(X: torch.Tensor, num_iters: int):\n",
        "    \"\"\"\n",
        "    Apply Newton-Schulz iterations to approximate orthogonalization.\n",
        "\n",
        "    This function applies the polynomial f(X) = (3X - X^3)/2 repeatedly to a normalized matrix,\n",
        "    which gradually forces all singular values to 1 while preserving singular vectors.\n",
        "\n",
        "    Args:\n",
        "      X (torch.Tensor): Input matrix to orthogonalize\n",
        "      num_iters (int): Number of Newton-Schulz iterations\n",
        "\n",
        "    Returns:\n",
        "      torch.Tensor: Orthogonalized matrix\n",
        "    \"\"\"\n",
        "    dtype = X.dtype\n",
        "    # Use bfloat16 for potential speed/memory savings during the iterations\n",
        "    X = X.bfloat16()\n",
        "    # Recall from prior homeworks that we can transpose the matrix to speed up computation.\n",
        "    transposed = False\n",
        "    if X.size(-2) < X.size(-1):\n",
        "        transposed = True\n",
        "        X = X.mT\n",
        "\n",
        "    # Ensure spectral norm is at most sqrt(3)\n",
        "    norm = torch.linalg.norm(X, dim=(-2, -1), keepdim=True)\n",
        "    X = torch.div(X, norm + 1e-7) * (3**0.5)\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "\n",
        "    if transposed:\n",
        "        X = X.mT\n",
        "\n",
        "    return X.to(dtype)"
      ],
      "metadata": {
        "id": "Mm5I-C5HIJQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "\n",
        "Notice that in the above implementation, we scale the spectral norm to be at most sqrt(3). **Can you explain why we choose this particular scaling?**\n",
        "\n",
        "*(Hint: Inspect the roots of the cubic polynomial. What is the connection between the roots and the convergence properties of the singular values? You can refer to Discussion 4 for the answer)*"
      ],
      "metadata": {
        "id": "7YEjXmzqS_Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Implementing Muon Update\n",
        "\n",
        "Now, we implement the update in a Muon optimizer. Given parameter matrix $W$ with momentum matrix $M$, the pseudocode for the Muon update proceeds as follows:\n",
        "\n",
        "```\n",
        "d_out, d_in = M.shape\n",
        "\n",
        "# Apply Newton-Schulz orthogonalization\n",
        "M ← newton_schulz_orthogonalize(M, ns_iters)\n",
        "        \n",
        "# Apply muP scaling factor for consistent update magnitude\n",
        "M ← M · sqrt(max(1, d_out / d_in))\n",
        "```\n",
        "\n",
        "Then, the Muon update is used later to update the parameters W:\n",
        "```\n",
        "# Update the parameter matrix\n",
        "W  ← W - lr * M\n",
        "```\n",
        "\n",
        "**Complete the following code** to implement the Muon update following the above pseudocode."
      ],
      "metadata": {
        "id": "YKyN6DSfaA7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def muon_update(grad, momentum, beta=0.95, ns_iters=5):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    ################################################################################\n",
        "    ################################################################################\n",
        "\n",
        "    # Restore shape if needed\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "\n",
        "class Muon(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, ns_iters=5,\n",
        "                weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, beta=beta, ns_iters=ns_iters,\n",
        "                        weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            ns_iters = group['ns_iters']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                # Get state for this parameter\n",
        "                state = self.state[p]\n",
        "                # Initialize momentum buffer if it doesn't exist\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                # Apply weight decay directly to parameters (AdamW style)\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Apply newton_schulz if parameter is a matrix\n",
        "                if p.ndim >= 2:\n",
        "                    update = muon_update(grad, state['momentum'],\n",
        "                                         beta=beta, ns_iters=ns_iters)\n",
        "                    # Apply update to parameters\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    # For non-matrix parameters, i.e. bias, use standard momentum update\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "k-m7S4cJnKx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "Note that Muon requires that parameters are 2D matrices of shape $d_{out} \\times d_{in}$. However, we know that parameters that are convolutional kernels have shape $c_{out} \\times c_{in} \\times k \\times k$ where $c$ denotes number of channels and $k$ is kernel size.\n",
        "\n",
        "Modern implementations of convolutional layers will transform an input image $\\mathbf{x}$ of shape $c_{in} \\times h \\times w$ to $\\mathbf{x}'$ such that each column has size $c_{in} \\cdot k \\cdot k$ and corresponds to one flattened \"receptive field\" of the image (or one patch of the image that a convolutional filter passes over to compute one pixel in the output).\n",
        "\n",
        "Given this fact, **how do we modify the convolutional kernel into a $d_{out} \\times d_{in}$ matrix $C$ such that the output of the convolutional layer can be expressed as $C \\mathbf{x}'$**.\n"
      ],
      "metadata": {
        "id": "F6V1-eIIsPrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Empirical Evaluation of Muon\n",
        "\n",
        "Now, we'll train the CNN network on the CIFAR10 dataset using our Muon implementation, comparing performance on the test set against other popular optimizers in SGD and AdamW.\n",
        "\n",
        "First, in addition to SGD and AdamW, we consider two additional baseline optimizers that will help us better interpret our results:\n",
        "\n",
        "\n",
        "*   MuonSVD: Rather than using Newton-Schulz, we orthogonalize the momentum using SVD on the momentum matrix $M = U\\Sigma V^T$ and computing $UV^T$.\n",
        "*   AdamWMuP: We add the muP scaling on top of the AdamW optimizer. This is meant the help us better understand how much of the Muon performance is due to the orthogonalization step, and how much is simply from the muP scaling.\n",
        "\n",
        "The cell below implements these two additional optimizers. You do not have to implement anything.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOa1BoaWeJ1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def muon_update_svd(grad, momentum, beta=0.95):\n",
        "    momentum.lerp_(grad, 1 - beta)  # momentum = beta * momentum + (1-beta) * grad\n",
        "    update = momentum.clone()\n",
        "\n",
        "    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "    original_shape = update.shape\n",
        "    reshaped = False\n",
        "    if update.ndim > 2:\n",
        "        reshaped = True\n",
        "        update = update.view(update.size(0), -1)\n",
        "\n",
        "    # Orthogonalization via SVD - specify full_matrices=False for reduced SVD\n",
        "    U, _, Vh = torch.linalg.svd(update, full_matrices=False)\n",
        "    update = torch.matmul(U, Vh)\n",
        "\n",
        "    # Apply muP scaling\n",
        "    update.mul_(max(1, update.size(-2) / update.size(-1))**0.5)\n",
        "\n",
        "    # Restore shape if needed\n",
        "    if reshaped:\n",
        "        update = update.view(original_shape)\n",
        "\n",
        "    return update\n",
        "\n",
        "\n",
        "class MuonSVD(optim.Optimizer):\n",
        "    def __init__(self, params, lr=0.01, beta=0.95, weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, beta=beta, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            beta = group['beta']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                # Get state for this parameter\n",
        "                state = self.state[p]\n",
        "                # Initialize momentum buffer if it doesn't exist\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(grad)\n",
        "\n",
        "                # Apply weight decay directly to parameters (AdamW style)\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                # Apply newton_schulz if parameter is a matrix\n",
        "                if p.ndim >= 2:\n",
        "                    update = muon_update_svd(grad, state['momentum'], beta=beta)\n",
        "                    # Apply update to parameters\n",
        "                    p.add_(update, alpha=-lr)\n",
        "                else:\n",
        "                    # For non-matrix parameters, i.e. bias, use standard momentum update\n",
        "                    momentum = state['momentum']\n",
        "                    momentum.mul_(beta).add_(grad)\n",
        "                    p.add_(momentum, alpha=-lr)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class AdamWMuP(optim.Optimizer):\n",
        "    def __init__(self,  params, lr=0.01, betas=(0.9, 0.999), weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    p.mul_(1 - lr * weight_decay)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1 - group[\"betas\"][0])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1 - group[\"betas\"][1])\n",
        "\n",
        "                m_hat = m / (1 - group[\"betas\"][0]**state['step'])\n",
        "                v_hat = v / (1 - group[\"betas\"][1]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                if p.ndim >= 2:\n",
        "                    # If the parameter is a convolutional kernel, then flatten to a 2D matrix\n",
        "                    original_shape = u.shape\n",
        "                    reshaped = False\n",
        "                    if u.ndim > 2:\n",
        "                        u = u.view(u.shape[0], -1)  # keep first dim, flatten the rest\n",
        "                        reshaped = True\n",
        "\n",
        "                    u.mul_(max(1, u.size(-2) / u.size(-1))**0.5)\n",
        "\n",
        "                    # Unflatten back to convolutional kernel\n",
        "                    if reshaped:\n",
        "                        u = u.view(original_shape)\n",
        "\n",
        "                p.add_(u, alpha=-lr)\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "Ix6NZCLkF7qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you performed hyperparameter sweeping (optional part below), then replace the default hyperparameters with the values you found from your sweep. Then, run the following cell to investigate how good Muon is relative to other baseline optimizers. The cell should take less than 10 minutes if you use a GPU runtime on Colab."
      ],
      "metadata": {
        "id": "nd48iWs2EoF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the optimizers you want to compare ---\n",
        "optimizers_dict = {\n",
        "    \"Muon\": lambda params: Muon(params, lr=1e-2, weight_decay=0),\n",
        "    \"SGD\": lambda params: torch.optim.SGD(params, lr=1e-2, momentum=0.9, weight_decay=1e-4),\n",
        "    \"AdamW\": lambda params: torch.optim.AdamW(params, lr=1e-3, weight_decay=1e-3),\n",
        "    \"MuonSVD\": lambda params: MuonSVD(params, lr=1e-2, weight_decay=0),\n",
        "    \"AdamWMuP\": lambda params: AdamWMuP(params, lr=1e-3, weight_decay=1e-3)\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "results = {}  # store loss curves\n",
        "accuracy_results = {}  # store accuracy curves\n",
        "\n",
        "# --- Train for each optimizer ---\n",
        "for opt_name, opt_fn in optimizers_dict.items():\n",
        "    print(f\"\\n--- Training with {opt_name} ---\")\n",
        "    model = CIFAR10CNN().to(device)  # re-init model each time\n",
        "    optimizer = opt_fn(model.parameters())\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(1, 6):  # Train for 5 epochs\n",
        "        model.train()\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/5], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        # Evaluate\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        accuracies.append(test_acc)\n",
        "\n",
        "        print(f\"{opt_name} | Epoch {epoch}, avg loss: {avg_loss:.4f}, test acc: {test_acc:.2f}%, time: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    results[opt_name] = losses\n",
        "    accuracy_results[opt_name] = accuracies\n",
        "\n",
        "    # Calculate and print total training time\n",
        "    total_time = sum(epoch_times)\n",
        "    print(f\"{opt_name} | Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# --- Plot results ---\n",
        "# 1. Loss vs Epoch\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "for opt_name, losses in results.items():\n",
        "    plt.plot(range(1, len(losses)+1), losses, label=opt_name, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss vs Epoch for Different Optimizers\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 2. Accuracy vs Epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "for opt_name, accuracies in accuracy_results.items():\n",
        "    plt.plot(range(1, len(accuracies)+1), accuracies, label=opt_name, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Test Accuracy vs Epoch for Different Optimizers\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c2SVu7okFWVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "\n",
        "**Which optimizer performed best between Muon, SGD, and AdamW?** Also **copy the resulting plots** into the submission as well."
      ],
      "metadata": {
        "id": "HCOdMfEFfHmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4\n",
        "\n",
        "Compare the loss curves between Muon and MuonSVD. **Are the results expected? Explain why.**"
      ],
      "metadata": {
        "id": "DqoNyPtLMf3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5\n",
        "\n",
        "The Muon optimizer contains two key differences: (1) orthogonalization of the momentum, and (2) muP scaling of the momentum. **Between orthogonalization and muP scaling, which seemed to matter more?** Reference the loss curves to justify your answer.  "
      ],
      "metadata": {
        "id": "_RVJi_lXM6JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6 (Optional)\n",
        "\n",
        "Our implementation of Newton-Schulz is suboptimal in the polynomial used for convergence.\n",
        "The community has developed quintic polynomials that converge faster while still being efficient. **Implement an improved Newton-Schulz and compare. Comment on the speed advantage of the improved Muon relative to the MuonSVD.**\n",
        "\n",
        "*(Hint: You can modify the number of iterations by setting the ns_iters parameter in the Muon optimizer)*"
      ],
      "metadata": {
        "id": "XyAeAbB2bssc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 (Optional): Hyperparameter sweeps\n",
        "\n",
        "To ensure that we are making fair comparisons, we will sweep over both learning rate and weight decay for both Muon and AdamW (the likely strongest competing optimizer). We choose these two parameters because empirically, they have the greatest effect on training.\n",
        "\n",
        "Running the sweep should take less than 1 hour total on a GPU runtime."
      ],
      "metadata": {
        "id": "oZ9TX-EqwdfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "def train_and_evaluate(optimizer_fn, lr, weight_decay, num_epochs=5):\n",
        "    \"\"\"Trains and evaluates the CIFAR10CNN with a given optimizer and hyperparameters.\"\"\"\n",
        "    model = CIFAR10CNN().to(device)\n",
        "    optimizer = optimizer_fn(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # Split training data into training and validation sets\n",
        "    train_size = int(0.8 * len(train_ds))\n",
        "    val_size = len(train_ds) - train_size\n",
        "    train_dataset, val_dataset = random_split(train_ds, [train_size, val_size])\n",
        "\n",
        "    train_loader_split = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for i, (x, y) in enumerate(train_loader_split):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        val_acc = evaluate(model, val_loader)\n",
        "        best_accuracy = max(best_accuracy, val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "# Define the optimizers and their hyperparameter search spaces\n",
        "optimizers_to_sweep = {\n",
        "    \"Muon\": {\n",
        "        \"optimizer_fn\": lambda params, lr, weight_decay: Muon(params, lr=lr, weight_decay=weight_decay),\n",
        "        \"lr_values\": [1e-2, 5e-3, 1e-3, 5e-4],\n",
        "        \"weight_decay_values\": [0, 1e-4, 1e-3]\n",
        "    },\n",
        "    \"AdamW\": {\n",
        "        \"optimizer_fn\": lambda params, lr, weight_decay: torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay),\n",
        "        \"lr_values\": [1e-2, 5e-3, 1e-3, 5e-4],\n",
        "        \"weight_decay_values\": [0, 1e-4, 1e-3]\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Perform the hyperparameter sweep for each optimizer\n",
        "for opt_name, opt_info in optimizers_to_sweep.items():\n",
        "    print(f\"\\n--- Hyperparameter sweep for {opt_name} ---\")\n",
        "    optimizer_fn = opt_info[\"optimizer_fn\"]\n",
        "    lr_values = opt_info[\"lr_values\"]\n",
        "    weight_decay_values = opt_info[\"weight_decay_values\"]\n",
        "\n",
        "    results[opt_name] = {}\n",
        "\n",
        "    for lr in lr_values:\n",
        "        for weight_decay in weight_decay_values:\n",
        "            print(f\"Training with lr={lr}, weight_decay={weight_decay}\")\n",
        "            accuracy = train_and_evaluate(optimizer_fn, lr, weight_decay)\n",
        "            results[opt_name][(lr, weight_decay)] = accuracy\n",
        "            print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Print the results and best hyperparameters for each optimizer\n",
        "print(\"\\n--- Hyperparameter Sweep Results ---\")\n",
        "for opt_name, opt_results in results.items():\n",
        "    print(f\"\\n{opt_name}:\")\n",
        "    for (lr, weight_decay), accuracy in opt_results.items():\n",
        "        print(f\"  (lr={lr}, weight_decay={weight_decay}): {accuracy:.2f}%\")\n",
        "\n",
        "    best_params = max(opt_results, key=opt_results.get)\n",
        "    print(f\"Best hyperparameters for {opt_name}: (lr={best_params[0]}, weight_decay={best_params[1]}) with validation accuracy {opt_results[best_params]:.2f}%\")"
      ],
      "metadata": {
        "id": "XLfdpF5h_ReG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7 (Optional)\n",
        "\n",
        "**What were the best choices for hyperparameters for Muon? What about for AdamW?**"
      ],
      "metadata": {
        "id": "PVQE4jRzDkU9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfWo6Br3fp8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}