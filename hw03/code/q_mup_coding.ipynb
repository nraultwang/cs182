{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scZxX6oEKzZn"
      },
      "source": [
        "## Maximal Update Parameterization\n",
        "\n",
        "In this problem, we will examine the training of a simple MLP with hidden layers of varying widths. We will then investigate the maximal update parameterization (muP) which will allow us to use a single global learning rate to jointly train layers of any width.\n",
        "\n",
        "Note: This homework question is new this year and it is messier than usual. We felt it was worth it to get it out so you can play with these new techniques. If you're feeling stuck, don't hesistate to ask questions on Ed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRp2k4Dbli_k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(all_hidden_sizes)-1):\n",
        "            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1]))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = []\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.sigmoid(layer(x))\n",
        "            activations.append(x)\n",
        "        x = self.layers[-1](x)\n",
        "        activations = activations[1:]\n",
        "        return x, [a.detach() for a in activations]\n",
        "\n",
        "# Load MNIST data\n",
        "(train_images, train_labels), (valid_images, valid_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "valid_images = valid_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_images = torch.from_numpy(train_images)\n",
        "train_labels = torch.from_numpy(train_labels).long()\n",
        "valid_images = torch.from_numpy(valid_images)\n",
        "valid_labels = torch.from_numpy(valid_labels).long()\n",
        "\n",
        "def rms(x, dim):\n",
        "    return torch.sqrt(torch.mean(x**2, dim=dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j68v19tfX5Vq"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleAdam(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "        b2: float = 0.999,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1, b2=b2,)\n",
        "        super(SimpleAdam, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1-group[\"b2\"])\n",
        "\n",
        "                m_hat = m / (1 - group[\"b1\"]**state['step'])\n",
        "                v_hat = v / (1 - group[\"b2\"]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                p.add_(u, alpha=-group['lr'])\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "XIYSY34qX80r",
        "outputId": "87e8d0cd-8806-47c9-f102-f1cd96d38506"
      },
      "outputs": [],
      "source": [
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "def train_one_step(mlp=MLP, hiddens=[8, 16, 64, 64, 64, 256, 256, 1024], optimizer=SimpleAdam, label=\"Adam\", lr=0.01):\n",
        "    model = mlp(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "\n",
        "    prev_activations = None\n",
        "    for i in range(2):\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i > 0:\n",
        "            print([a.shape for a in activations])\n",
        "            activation_deltas = [a - pa for a, pa in zip(activations, prev_activations)]\n",
        "            activation_deltas_rms = [torch.mean(rms(a, dim=-1)) for a in activation_deltas]\n",
        "        prev_activations = activations\n",
        "\n",
        "    # plot deltas\n",
        "    deltas = np.array(activation_deltas_rms)\n",
        "    fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "    axs.set_title(f'RMS of activation deltas per layer ({label})')\n",
        "    axs.set_xlabel('Hidden Size of activation')\n",
        "    axs.bar(np.arange(deltas.shape[0]), deltas)\n",
        "    axs.set_xticks(np.arange(deltas.shape[0]))\n",
        "    axs.set_xticklabels(hiddens[1:])\n",
        "    plt.show()\n",
        "train_one_step(optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykmT0CjoLaCH"
      },
      "source": [
        "## a. Examining the norms of a heterogenous MLP.\n",
        "\n",
        "Run the above cell, which trains a neural network for a single gradient step, then examines the effect of that step on the resulting activations. What are the dimensions of each layer in the neural network? How does the dimensionality of the layer affect the RMS norm of the activation deltas? Change the widths of some of your neural network layers, and recreate the plot -- did the RMS values change as expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "s0-GhF6eNN5I",
        "outputId": "8d21a6b9-8027-49a6-c6b5-a1843b1b50e5"
      },
      "outputs": [],
      "source": [
        "# TODO: Call some plotting code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzosCNjMlMZ"
      },
      "source": [
        "## b. Examining the norms of the updates to the weights.\n",
        "\n",
        "In the provided code above, we plotted the change in norms of the *activation vectors*. Now, you will examine the change in the weights themselves. Create a version of the above function that runs a single gradient step, then for each dense layer plot:\n",
        "- The *Frobenius* norm of the update.\n",
        "- The *spectral* norm of the update.\n",
        "- The *RMS-RMS induced norm* of the update.\n",
        "\n",
        "Which one of these norms correlates the most with the RMS norms of the activations?\n",
        "\n",
        "You should calculate your updates as `new_dense_parameter - old_dense_parameter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "lOp2ISs2NfA9",
        "outputId": "db77a082-8aeb-4328-8185-5325d85d0eb8"
      },
      "outputs": [],
      "source": [
        "### Solution\n",
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "def train_one_step_matrices(mlp=MLP, hiddens=[8, 16, 64, 64, 64, 256, 256, 1024], optimizer=SimpleAdam, label=\"Adam\", lr=0.01):\n",
        "    model = mlp(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "\n",
        "    old_params = [p.detach().clone() for p in model.parameters()]\n",
        "\n",
        "    for i in range(1):\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    new_params = [p.detach().clone() for p in model.parameters()]\n",
        "    delta_params = [new_p - old_p for new_p, old_p in zip(new_params, old_params)]\n",
        "\n",
        "    frob_norms = []\n",
        "    spectral_norms = []\n",
        "    induced_norms = []\n",
        "    p_shapes = []\n",
        "\n",
        "    for p in delta_params:\n",
        "        if len(p.shape) == 2:\n",
        "            ### TODO: Log the respective norms.\n",
        "            pass\n",
        "            ###\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    axs[0].set_title(f'Frobenius norm of update per layer ({label})')\n",
        "    axs[0].set_xlabel('Hidden Size of layer')\n",
        "    axs[0].bar(np.arange(len(frob_norms)), frob_norms)\n",
        "    axs[0].set_xticks(np.arange(len(frob_norms)), p_shapes, rotation=45, ha='right')\n",
        "\n",
        "    axs[1].set_title(f'Spectral norm of update per layer ({label})')\n",
        "    axs[1].set_xlabel('Hidden Size of layer')\n",
        "    axs[1].bar(np.arange(len(spectral_norms)), spectral_norms)\n",
        "    axs[1].set_xticks(np.arange(len(spectral_norms)), p_shapes, rotation=45, ha='right')\n",
        "\n",
        "    axs[2].set_title(f'Induced norm of update per layer ({label})')\n",
        "    axs[2].set_xlabel('Hidden Size of layer')\n",
        "    axs[2].bar(np.arange(len(induced_norms)), induced_norms)\n",
        "    axs[2].set_xticks(np.arange(len(induced_norms)), p_shapes, rotation=45, ha='right')\n",
        "    plt.show()\n",
        "train_one_step_matrices(optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYleV00ERYKW"
      },
      "source": [
        "## c. Implementing muP\n",
        "\n",
        "We will now implement muP scaling. Modify the starter code below to set a per-layer learning rate such that the resulting RMS activation-deltas are uniform scale, regardless of the layer widths. Plot the resulting activation-deltas on at least two sets of widths.\n",
        "\n",
        "Note: Even with the correct scaling, the first 2-3 activation-deltas may have a lower norm than the rest. Can you think of a reason why this might be the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "Kq8g0H4bYF2W",
        "outputId": "2f14eb42-fb8d-4f7f-f469-b3fdcb34064f"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleAdamMuP(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "        b2: float = 0.999,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1, b2=b2,)\n",
        "        super(SimpleAdamMuP, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "                    state['variance'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "                v = state['variance']\n",
        "                v.lerp_(grad**2, 1-group[\"b2\"])\n",
        "\n",
        "                m_hat = m / (1 - group[\"b1\"]**state['step'])\n",
        "                v_hat = v / (1 - group[\"b2\"]**state['step'])\n",
        "                u = m_hat / (torch.sqrt(v_hat) + 1e-16)\n",
        "\n",
        "                ############################\n",
        "                ### Todo: Adjust the per-layer learning rate scaling factor so per-layer RMS activation deltas are constant.\n",
        "                ### Hint for part e: The following tricks will help you retain performance when using muP scaling.\n",
        "                ###  - Treat biases as a hidden layer with size (d_out, 1). You will need to use a fudge-factor of around 0.01 -- we want to keep the change in bias terms low.\n",
        "                ###  - For the input layer, a fudge factor of 10 appears to help.\n",
        "                ###  - For the output layer, we find it is best to ignore the muP scaling, and instead use a fixed learning rate (e.g. 0.003).\n",
        "                ############################\n",
        "                lr = group['lr']\n",
        "                pass\n",
        "                ############################\n",
        "                ############################\n",
        "\n",
        "                p.add_(u, alpha=-lr)\n",
        "        return None\n",
        "train_one_step(optimizer=SimpleAdamMuP, lr=2, label=\"Adam MuP\", hiddens=[8, 16, 64, 64, 64, 256, 256, 1024])\n",
        "train_one_step(optimizer=SimpleAdamMuP, lr=2, label=\"Adam MuP\", hiddens=[8, 16, 32, 64, 128, 256, 512, 1024])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GznnkrDAjV1e"
      },
      "source": [
        "## d. Per-Weight Multipliers\n",
        "\n",
        "An alternative way to implement muP is to adjust the *network graph* itself, rather than the optimizer. Implement this below, and recreate the above uniformly-scaled graph when using the *Adam* (not muP) optimizer. We have disabled biases to simplify the problem.\n",
        "\n",
        "Why is multiplying the output of a layer by a constant the same as adjusting the learning-rate of that layer (when using Adam or SignGD)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "9XueS5-OjoMh",
        "outputId": "0e730124-22e4-41ec-9417-ba2604613de6"
      },
      "outputs": [],
      "source": [
        "class ScaledMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n",
        "        super(ScaledMLP, self).__init__()\n",
        "        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(all_hidden_sizes)-1):\n",
        "            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = []\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer(x)\n",
        "            ## TODO\n",
        "            pass\n",
        "            ##\n",
        "            x = self.sigmoid(x)\n",
        "            activations.append(x)\n",
        "        x = self.layers[-1](x)\n",
        "        activations = activations[1:]\n",
        "        return x, [a.detach() for a in activations]\n",
        "\n",
        "train_one_step(mlp=ScaledMLP, optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHHRVpQMiEvS"
      },
      "source": [
        "## e. Hyperparameter Transfer\n",
        "\n",
        "Run the following code, which will perform a sweep over learning rates for 3-layer MLPs of increasing width using Adam. How does the optimal learning rate change as the network increases in size?\n",
        "\n",
        "In the second cell, we will instead use the muP optimizer you implemented. How does the optimal learning rate work now? You should aim to show that there is a single global learning rate that works on a majority of widths. The 256-width network should achieve a loss of 0.5, comparable to Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_z3UhD8Ytts"
      },
      "outputs": [],
      "source": [
        "valid_idx = np.random.randint(0, len(train_images), size=64)\n",
        "valid_images = train_images[valid_idx]\n",
        "valid_labels = train_labels[valid_idx]\n",
        "valid_images, valid_labels = valid_images.to(device), valid_labels.to(device)\n",
        "\n",
        "\n",
        "def train_with_lr(hiddens=[64, 64, 64], optimizer=SimpleAdam, lr=0.01):\n",
        "    torch.manual_seed(4)\n",
        "    np.random.seed(4)\n",
        "    model = MLP(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(100):\n",
        "        batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_valid, _ = model(valid_images)\n",
        "            valid_losses = criterion(outputs_valid, valid_labels)\n",
        "            losses.append(valid_losses.item())\n",
        "\n",
        "    return np.mean(np.array(losses)[-5:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "dEIPwU8SZtIz",
        "outputId": "c9ac0405-54a6-427f-f810-345d91a9ea29"
      },
      "outputs": [],
      "source": [
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]\n",
        "adam_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        loss = train_with_lr(hiddens=[width, width, width], lr=lr)\n",
        "        adam_results[wi, lri] = loss\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), adam_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "cTHetldFdMOP",
        "outputId": "d90a7f8b-7191-4b1a-e44b-4c4cce3db41d"
      },
      "outputs": [],
      "source": [
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.03, 0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "mup_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        loss = train_with_lr(hiddens=[width, width, width], lr=lr, optimizer=SimpleAdamMuP)\n",
        "        mup_results[wi, lri] = loss\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), mup_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaqzanKomMTx"
      },
      "source": [
        "## e. Shampoo and Orthogonalization\n",
        "\n",
        "In lecture, we discussed a simplified version of the Shampoo update, which can be viewed as *orthogonalizing* the update to a dense layer. In the following code block, implement this simplified Shampoo update:\n",
        "\n",
        "$$\n",
        "momentum \\rightarrow U \\Sigma V^T. \\qquad update = UV^T.\n",
        "$$\n",
        "\n",
        "Feel free to use linear algebra functions such as `torch.linalg.svd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUgzUqQ9nALJ"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleShampoo(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1)\n",
        "        super(SimpleShampoo, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "\n",
        "                ############ TODO\n",
        "                if len(m.shape) == 1:\n",
        "                    u = m # Ignore biases for this question, it's not important.\n",
        "                else:\n",
        "                    pass\n",
        "                #############\n",
        "                p.add_(u, alpha=-group['lr'])\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoTUgUYmvJt"
      },
      "source": [
        "Now, we will examine the relationship between the Frobenius norm and the Spectral norm for Adam vs. Shampoo. Plot these norms using your code from part c. What relationship do you see? Can you come up for a reason why this makes sense?\n",
        "\n",
        "Bonus: How should we scale the Shampoo update so the *induced RMS-RMS norm* is equal? Implement this change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2pcGr3WloaGw",
        "outputId": "0eb1e796-4529-4f1b-aae7-8fb91a5c9be4"
      },
      "outputs": [],
      "source": [
        "train_one_step_matrices(optimizer=SimpleAdam)\n",
        "train_one_step_matrices(optimizer=SimpleShampoo, label=\"Shampoo\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
